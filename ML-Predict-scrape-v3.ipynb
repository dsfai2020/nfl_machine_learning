{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape 2019\n",
    "\n",
    "Gets the NFL schedule for 2019. Interested in Week#, Game#, Ateam(home team), Bteam(visiting team), Date of game.\n",
    "\n",
    "Data Source: http://www.nfl.com/schedules/2019/REG1\n",
    "\n",
    "Updates scoresPlus with basic information on the schedule information for each scheduled game;\n",
    "that is, year, week, game, aTeam, bTeam, and winner (='predict')\n",
    "\n",
    "\n",
    "?????   Additionally, update to current year's quarterback and ranking: https://www.thedelite.com/2019-nfl-starting-quarterback-rankings/32/  **** add this \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key inputs (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearX = '2019'\n",
    "weekX = '12'                      # the week for new schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, etc.\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" \n",
    "    Create a database connection to a SQLite database.\n",
    "    Note: When you connect to an SQLite database file that does not exist, SQLite creates a new database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print('Database created:', db_file, ';  Sqlite3 version:', sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created: NFL.sqlite ;  Sqlite3 version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# This will connect to file, if it exists.\n",
    "NFLdb = \"NFL.sqlite\"\n",
    "create_connection(NFLdb)\n",
    "conn = sqlite3.connect(NFLdb)         # make  “connection” to the database\n",
    "cur = conn.cursor()                            # cursor is like a ﬁle handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nfl.com/schedules/2019/REG12\n",
      "Title: <title>NFL 2019 Regular Season Week 12 Schedule - NFL.com</title> ****Week: 12\n",
      "original dump: 235456\n",
      "\n",
      "Year, Week,Game,bteam,ateam: \n",
      "2019 12 1 Colts Texans\n",
      "2019 12 2 Broncos Bills\n",
      "2019 12 3 Giants Bears\n",
      "2019 12 4 Steelers Bengals\n",
      "2019 12 5 Dolphins Browns\n",
      "2019 12 6 Buccaneers Falcons\n",
      "2019 12 7 Panthers Saints\n",
      "2019 12 8 Seahawks Eagles\n",
      "2019 12 9 Lions Redskins\n",
      "2019 12 10 Raiders Jets\n",
      "2019 12 11 Jaguars Titans\n",
      "2019 12 12 Cowboys Patriots\n",
      "2019 12 13 Packers 49ers\n",
      "2019 12 14 Ravens Rams\n"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in ['\"','\\n']:\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "\n",
    "def extractWK(yr_x, wk_x):\n",
    "    \"\"\"\n",
    "    Extracts weekly game data from ESPN website (ususally 16 games)\n",
    "    Input is year and week #\n",
    "    Updates scoresPlus with: year, week, game, aTeam, and bTeam\n",
    "    Returns none\n",
    "    \"\"\"\n",
    "        \n",
    "    url = 'http://www.nfl.com/schedules/2019/REG'  + wk_x \n",
    "    print(url)\n",
    "    \n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(\"Title:\", soup.title, '****Week:', wk_x)                      # print title\n",
    "    dump = soup.get_text()                        # dump data; print it to study data; also, print(soup.prettify())\n",
    "    print('original dump:', len(dump))\n",
    "\n",
    "    dump = clean_text(soup)\n",
    "    temp = re.sub(r'\\b(a|an|the)\\b', ' ', dump)   # get rid of special articles, etc.\n",
    "    temp = temp.replace('49er', 'Xxer')          # make 49ers recognizable\n",
    "    temp = re.findall('\\|([A-Z][a-z]+\\_[A-Z][a-z]+)?\\|[A-Z]{2,}', temp)\n",
    "    print ('\\nYear, Week,Game,bteam,ateam: ')\n",
    "    for idx, item in enumerate(temp):  \n",
    "        if 'Xxers' in item:\n",
    "            item = item.replace('Xxers', '49ers')\n",
    "        bteam = item.split('_')[0]\n",
    "        ateam = item.split('_')[1]\n",
    "        game = idx+1\n",
    "        print(yr_x, wk_x, game, bteam, ateam)\n",
    "        \n",
    "        # update database with basic data for this game \n",
    "        winner = 'predict'\n",
    "        \n",
    "        cur.execute('INSERT or REPLACE INTO scoresPlus (year,week,game, aTeam, bTeam, winner) VALUES (?,?,?,?,?,?)',\n",
    "                (yr_x, wk_x, game, ateam, bteam, winner) )\n",
    "\n",
    "    return \n",
    "\n",
    "extractWK(yearX, weekX)\n",
    "conn.commit()                     # Make sure it is flushed to database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
